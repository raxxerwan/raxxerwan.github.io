---
layout: post
title: 【Paper Reading】MSNet
---

[**MotionSqueeze: Neural Motion Feature Learning for Video Understanding**](https://arxiv.org/abs/2007.09933v1)

# 动机

MSNet想要解决的问题仍然是如何在2D网络中建模时序。最直观也是最原始的方法是双流输入，让网络从光流输入中提取时序信息。但是光流提取太慢，即使使用FlowNet来提光流，也会给2D网络带来很多累赘。MSNet提取了FlowNet中的一些关键单元，develop了一个MotionSqueeze Module。这个模块可以加到TSN网络中，使模型具有一定的（类似于光流的）对时序建模的能力。

# 结构

## MotionSqueeze Module（MS模块）

MS模块分三块。

![correlation](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/MSModule.JPG)

### 1.correlation layer

correlation layer出自于FlowNet。输入两个特征图，它能够计算这两个特征图之间各个点之间的相似性，输出一张特征图。这个特征图上任意一点的计算如下：

![correlation](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/correlation.JPG)

也就是s(x,p,t)计算的是输入特征图Ft在x处的特征与输入特征图F(t+1)在x+p处的特征的相似度（点积）。<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\mathbf{p}\in&space;{[k,k]}^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\mathbf{p}\in&space;{[k,k]}^2" title="\mathbf{p}\in {[k,k]}^2" /></a>，也就是s是个<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;(T-1)&space;\times&space;H&space;\times&space;W&space;\times&space;p^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;(T-1)&space;\times&space;H&space;\times&space;W&space;\times&space;p^2" title="(T-1) \times H \times W \times p^2" /></a>的特征图。correlation layer的实现在github上有现成的pytorch并行版本，类似于用F(t+1)作为卷积核对Ft进行卷积，但是更复杂一些。

### 2.displacement estimation

displacement类似于光流，但是比光流的信息要粗糙得多。

计算方式如下：

1. 首先给correlation乘一个mask g；这个mask是用高斯函数算出来的，均值在<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{argmax}_\mathbf{p}s(\mathbf{x},\boldsymbol{\mathbf{}p},t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{argmax}_\mathbf{p}s(\mathbf{x},\boldsymbol{\mathbf{}p},t)" title="{argmax}_\mathbf{p}s(\mathbf{x},\boldsymbol{\mathbf{}p},t)" /></a>。注意指数是个二维坐标，算出来的g(x,p,t)是一个标量。p离最大响应值的坐标越远，g就越小，即被抑制；反之g大，被增强。
2. 再用kernal-soft-argmax得到粗糙版的光流raw-displacement，记为d。注意d(x,t)是一个二维坐标，也就是d应该是个(T-1) * H * W * 2的矩阵，类似于光流。

![form1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/form1.JPG)

3. 实际上还要再给raw-displacement concat一个confidence map，给予displacement更多的信息。confidence map如下：

![form2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/form2.JPG)

4. concat raw-displacement和confidence map，得到(T-1) * H * W * 3的最终displacement D。

### 3.feature transformation

多个2D卷积层，目的是升通道到C。注意这些2D卷积层都是depth-wise separable convolution，目的是为了降低参数量。最终得到MSModule的输出M。

## 把MS加到backbone中

![structure](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/structure.JPG)

## 怎么加，加在哪

最好的效果是只加在res3的最后一个block后面。（说明这个module设计得确实不太好，没抓到本质）

# 实验

## inference

**Kinetics400：** 10clips，中心crop

**Something-Something：** 单clip/10clips，中心crop

## performence

![p1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/performence1.JPG)

![p2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/performence2.JPG)

额外参数量确实引入的不多。

在Something-Something上似乎吊打了TDRL，在FLOPS接近的情况下，比TDRL高了1-2个点。

在Kinetics上inference方式不同，不好评价，但是在views数只是10的情况下，只比views数为30的TDRL低了0.5个点（FLOPS接近），还是很有说服力的。说明这是个可行的方向。

## runtime

![runtime](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/runtime.JPG)


# 探究

![ablation](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/ablation.JPG)

## displacement的一些变量

如Table 3所示，S是softargmax，KS是kernal-softargmax；CM是confidence map，BD是backward displacement tensor，是在计算correlation的时候把时序颠倒。可以看到KS确实明显比S要好，且加入更多的信息（CM，BD）确实更有助于时序建模。

## 可视化

![visulization](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-24-PR-MSNet/visulization.JPG)

可以看到displacement确实是和光流提取有类似之处的，而且其确实inherently地去掉了特征图中的背景信息，和TDRL想要达到的目的是一样的。




