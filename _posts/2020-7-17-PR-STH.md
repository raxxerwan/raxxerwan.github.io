---
layout: post
title: 【Paper Reading】STH
---

[**STH: Spatio-Temporal Hybrid Convolution for Efficient Action Recognition**](https://arxiv.org/pdf/2003.08042v1.pdf)

# 动机

在视频动作识别任务中，在相同的backbone下，3D网络的性能一般好于2D网络。其主要原因在于3D网络的3D卷积核能够更好的对时序信息进行建模。然而3D卷积核的超多参数量及FLOPS又使其相对于2D网络要难训练得多。而STH在2D卷积核中加入一定比例的3D卷积核，类似于分组卷积，在几乎不影响参数量（甚至减少参数量）的情况下使2D网络具有更好的时序信息建模能力。

# 结构

STH在实验中沿用了TSN-ResNet的2D框架，只是将其中的3 * 3卷积核替换成STH卷积核。

## STH卷积核

在一般的2D卷积中，如果输入特征图的size是W * H * Ci，输出特征图的size是W * H * Co，那么一个3 * 3的2D卷积层可以看成是用Co个size为3 * 3 * Ci的卷积核依次作用于该特征图。而STH的做法则是将每个3 * 3 * Ci卷积核在通道维度上进行拆解，拆解成一部分是1 * 3 * 3 * C1的spatial卷积核，一部分是3 * 1 * 1 * C2的temporal卷积核，组合在一起即是“STH卷积核”，其中C1+C2=Ci。这种做法其实类似于分组卷积，一些组用spatial卷积，一些组用temporal卷积。

而我们知道有Co个卷积核需要进行这样的拆解，STH对这Co个卷积核进行拆解的方式也略有不同，如图所示：

![STH卷积核](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/STH-conv.JPG)

即3 * 1 * 1 * C2的temporal卷积核的位置是不同的。可以看作是将temporal卷积核在STH卷积核上沿着通道维度平移。

这样一来，设C2 = p * C， C1 = (1-p) * C，STH卷积核的参数量为p * 3 * 1 * 1 * C + (1-p) * 1 * 3 * 3 * C。不难发现，p取得越大，STH卷积核的参数量越少。文章发现当p=1/4时，模型的表现最好。

## STH卷积核的变种

STH卷积核可以分成<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{STH}_{merge}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{STH}_{merge}" title="{STH}_{merge}" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{STH}_{hybird}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{STH}_{hybird}" title="{STH}_{hybird}" /></a>两种形式。其中hybird形式即为之前所述的形式。merge形式是STH的一种更加方便实现的形式，其缺点是输出特征图中只有部分通道融合了temporal信息。虽然STH还没有公开源码，但是我可以猜测一下hybrid和merge分别的实现方式。
![STH卷积核变种](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/STH-variant.JPG)
文章通过实验发现<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{STH}_{hybird}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{STH}_{hybird}" title="{STH}_{hybird}" /></a>的效果更好。

## 两种STH卷积核的实现

基于pytorch
```python
# STH merge
class STHConv_merge:
    def __init__(self, Ci, Co):
        self.conv_T = torch.nn.Conv3d(Ci, Co // 4, (3,1,1))
        self.conv_S = torch.nn.Conv3d(Ci, Co * 3 // 4, (1,3,3))

    def forward(self, x): # x: [B, Ci, T, H, W]
        B, Ci, T, H, W = x.shape
        O_T = self.conv_T(x) # [B, Co/4, T, H, W]
        O_S = self.conv_S(x) # [B, Co*3/4, T, H, W]
        return torch.cat((O_T, O_S]), dim=1) # [B, Co, T, H, W]

# STH hybrid
class STHConv_hybrid:
    def __init__(self, Ci, Co):
        self.conv_T = torch.nn.Conv3d(Ci, Co, (3,1,1), groups=4)
        self.conv_S_1 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))
        self.conv_S_2 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))
        self.conv_S_3 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))
        self.conv_S_4 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))

    def forward(self, x): # x: [B, Ci, T, H, W]
        B, Ci, T, H, W = x.shape

        O_T = self.conv_T(x) # [B, Co, T, H, W]

        x1 = x[:,Ci//4:, :, :, :] # [B, Ci/4, T, H, W]
        x2 = torch.cat((x[:,:Ci//4, :, :, :], x[:,Ci*2//4:, :, :, :], dim=1) # [B, Ci/4, T, H, W]
        x3 = torch.cat((x[:,:Ci*2//4, :, :, :], x[:,Ci*3//4:, :, :, :], dim=1) # [B, Ci/4, T, H, W]
        x4 = x[:,:Ci*3//4, :, :, :] # [B, Ci/4, T, H, W]
        O_S_1 = self.conv_S_1(x1) # [B, Co/4, T, H, W]
        O_S_2 = self.conv_S_2(x2) # [B, Co/4, T, H, W]
        O_S_3 = self.conv_S_3(x3) # [B, Co/4, T, H, W]
        O_S_4 = self.conv_S_4(x4) # [B, Co/4, T, H, W]
        O_S = torch.cat((O_S_1, O_S_2, O_S_3, O_S_4), dim=1) # [B, Co, T, H, W]
        return O_S + O_T
```

## temporal信息与spatial信息的attention融合

如果我们把spatial卷积核卷出来的特征图和temporal卷积核卷出的的特征图分开来看，分别记作<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{O}^{S}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{O}^{S}" title="{O}^{S}" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{O}^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{O}^{T}" title="{O}^{T}" /></a>。之前所述的默认做法其实是将这两个特征图直接element-wise相加，得到的特征图就是STH卷积核卷出的特征图。但是如果我们想进一步增强模型对时序信息和空间信息的权衡，可以不做element-wise相加，而是attention融合：

![融合1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/integration1.JPG)

attention参数的计算如下图所示：

![融合2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/integration2.JPG)

后文中，文章用了一个很有意思的方式来验证attention的重要性。

# 实验

## inference

**Something-Something V1:** 中心crop，单clip

**Kinetics400&UCF101&HMDB（不依赖时序信息数据集）:** 10crop，10clip

**Something-Something V2&Jester（依赖时序信息数据集）:** 中心crop，2clip

## performence

![p1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/performence1.JPG)

![p2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/performence2.JPG)

![p3](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/performence3.JPG)

在大部分依赖时序信息的数据集上，STH做到了当时的SOTA，且FLOPS和Params显著的低，甚至低于TSN。只在Jester的top 1上略低于TSM。

在Kinetics上，仍然打不过3D网络。

## runtime

![runtime](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/runtime.JPG)

尽管FLOPS数低，但是由于卷积形状的特殊性，目前肯定做不到很好的加速兼容，因此速度略逊于TSM。

## Attention

文章统计并可视化了不同情况下的attention系数的分布情况（计算方式是简单的将attention系数向量加和平均），来观察模型对于时序信息和空间信息的注意力分布。

![att1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/att1.JPG)

![att2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/att2.JPG)


可视化结果令人舒适，可得出如下结论：
1. **模型层数越深，时序信息越重要（<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{\alpha}^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{\alpha}^{T}" title="{\alpha}^{T}" /></a>的求和平均越大）**
2. p越小，即STH卷积核中的temporal卷积核比重越低，模型越会倾向于更多地关注时序信息来补足temporal卷积核的匮乏。
3. 在越依赖时序信息的数据集上，模型越关注时序信息。
4. 当使用光流输入时，模型比起使用RGB输入更关注时序信息。