---
layout: post
title: 【Paper Reading】Temporal Pyramid Network for Action Recognition
---

**TPN**

## 动机

TPN主要是为了解决视频动作识别中的许多动作的类内temporal信息差异过大的问题（比如同样是跑步动作，有的样本很快，有的样本较慢）。

用CNN网络中的不同Stage的输出特征图做不同尺度的采样，得到金字塔结构的信息，最后用这些不同时间尺度的信息进行融合。


## 结构

![结构](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-12-PR-TPN/TPN_structure.JPG)

### 输入

输入的典型值为32 * 2和8 * 8，即选取视频等间隔的64帧，然后取32或者8输入网络。

### Backbone

文章选了SlowFast中的Slow网络作为3D backbone，以及TSN作为2D backbone。

**3D backbone**: ResNet50。分别只把res4和res5的第一层1 * 1卷积核inflate为3 * 1 * 1。不进行时间维度的下采样，即t的stride都是1。详见SlowFast。

**2D backbone**: TSN-ResNet50，没什么特别的。

### Spatial Semantic Modulation

编码不同depth的spatial信息。方法是对某一stage的输出特征图进行二维卷积。通过在不同的stage选取不同stride的卷积核，获得相同尺寸的编码结果。同时，用于编码不同stage的卷积核的数量是一样的（文章选取1024）。这样保证了对每个stage的编码结果都是一模一样的尺寸（T * 7 * 7 * 1024）。

### Temporal Rate Modulation

对SSM的编码结果进行简单的时间维度下采样。越深的depth用越高的采样率。Typically，最深的SSM的编码结果下采样为T=1，倒数第二深的下采样为T=2，以此类推。

### Information Flow

对不同depth的编码结果进行融合，其实就是累加。四种累加方式（自顶向下，自底向上，两者并联，两者串联）。因为不同depth的编码结果的T不一样，因此为了能够累加会作上/下采样匹配尺寸。

![累加](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-12-PR-TPN/accmu.JPG)

![四种累加方式](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-12-PR-TPN/IF.JPG)

## 探究

### 用哪些depth的特征图？

文章发现只用res4和res5的特征图进行编码效果最好。用浅层的特征图会有不同程度的掉点。这和SlowFast的结果是一样的。SlowFast的Slow网络只在res4和res5做了卷积核的inflate，也就是只在这两个stage提取temporal信息，否则就会掉点。

但是我认为这里没必要。因为SlowFast里Slow网络的输入是稀疏采样，前后帧间隔较远，因此浅层的时候，由于spatial感受野不够大，前后帧的temporal关联比较小，才只在深层的时候inflate卷积核。而这篇除了后面的ablation study，都在用32帧的输入，已经不是稀疏采样了，完全可以看看对浅层的卷积核也进行inflate，同时对浅层的stage的特征图也进行编码的效果如何。

### 输入帧数对模型的影响？

浅层模型（ResNet50）对稠密输入的适应性比深层模型（ResNet101）要差。在ResNet50中，输入从8帧到32帧，先涨点后掉点。在ResNet101中，输入从8帧到32帧，持续涨点。但是装备了TPN后，ResNet50和101都随着输入帧数的增加而持续涨点。

### 只用一个depth的特征图进行编码怎么样？

不怎么样。文章尝试只使用最后一个stage的输出特征图，进行不同采样率的时间下采样。结果掉点。

### 哪种累加方式最好？

并联累加。

## 表现

![表现](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-12-PR-TPN/performence.JPG)

虽然宣称在K400（TPN-R101）和S-S（TSM+TPN）上达到了SotA，但是比较方式似乎不太公平。

在K400上，比较的对象是Slow输入只有8帧的SlowFast。而TPN-ResNet则使用了32帧的输入才超过SlowFast。虽然SlowFast的Fast输入有64帧，但是其FLOPS大部分都集中在Slow网络中。也就是即使不算TPN额外引入的FLOPS，TPN光backbone的FLOPS就大概是相同backbone的SlowFast的网络的4倍。加上TPN，FLOPS就更大了，和SlowFast完全不是一个量级的网络。另外TPN还在Temporal Rate Modulation额外引入了几个卷积层，参数量也有一定增加。

不过在S-S数据集上，加入了TPN的TSM确实是涨点明显，这个SotA没得黑。
