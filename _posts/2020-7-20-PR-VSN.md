---
layout: post
title: 【Paper Reading】VSN
---

[**Learning Efficient Video Representation with Video Shuffle Networks**](https://arxiv.org/pdf/1911.11319.pdf)

# 动机

这一篇的工作主要是在TSM的基础上进行改进。在TSM中，当前帧只能融合前后两帧的信息，VSN通过对所有帧的通道进行shuffle，使得当前帧能够看到前后所有帧的信息。

# 结构

VSN在实验中沿用了TSN-ResNet的2D框架，只是将其中部分block插入通道shuffle模块。

## 通道Shuffle和Restore

![structure](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/structure.JPG)

做法很简单，如图所示，在ResNet50的3 * 3卷积前后分别加上通道的shuffle和restore，使得输入在进行卷积之前能够获得其它帧的信息，卷积结束之后再将打乱的通道恢复。

## 怎么加

在什么地方shuffle，什么地方restore？文章给出了两种方法。

![how](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/how.JPG)

**compact：** 在一个瓶颈block的3 * 3卷积层的前后分别shuffle和restore。

**headtail：** 在一个瓶颈block的前后分别shuffle和restore。

文章实验得出，compact方法得到的结果更好。

## 加在哪

在每个Stage的最后一个block中加入shuffle和restore，其它的block都沿用TSM设计。

# 实验

## inference

单crop，单clip，8帧/clip。

## performence

![p1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/performence1.JPG)

![p2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/performence2.JPG)

在依赖时序信息的数据集上，VSN都刷到了当时的SotA，且FLOPS和参数量对比同backbone的TSN都没有增加。

在Kinetics上，仍然打不过SlowFast。但是值得注意的是，双流版本的VSN已经非常接近单RGB输入的NL-I3D。

## runtime

速度方面，在同样的ResNet50 backbone下，VSN与TSM的速度相当，甚至更快。

![p3](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/performence3.JPG)

## 光流

文章做了一个实验，来说明VSN能够使模型对更好地使用光流信息。

![光流](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/opticalflow.JPG)

可以看到，在仅仅使用光流作为输入地情况下，对比TSN，光流stack的帧数越少，VSN的涨点越明显。也就是说，VSN使得模型能够在很少的光流输入中提出很多的信息，因此对于VSN来说，光流stack的帧数使用5还是1的区别就没那么大了。
