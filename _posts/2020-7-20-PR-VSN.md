---
layout: post
title: 【Paper Reading】VSN
---

[**Learning Efficient Video Representation with Video Shuffle Networks**](https://arxiv.org/pdf/1911.11319.pdf)

# 动机

这一篇的工作主要是在TSM的基础上进行改进。在TSM中，当前帧只能融合前后两帧的信息，VSN通过对所有帧的通道进行shuffle，使得当前帧能够看到前后所有帧的信息。

# 结构

VSN在实验中沿用了TSN-ResNet的2D框架，只是将其中部分block插入通道shuffle模块。

## 通道Shuffle和Restore

![structure](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/structure.JPG)

做法很简单，如图所示，在ResNet50的3 * 3卷积前后分别加上通道的shuffle和restore，使得输入在进行卷积之前能够获得其它帧的信息，卷积结束之后再将打乱的通道恢复。

## compact和headtail

在什么地方shuffle，什么地方restore？文章给出了两种方法。

![how](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-20-PR-VSN/how.JPG)

**compact：** 在一个瓶颈block的3 * 3卷积层的前后分别shuffle和restore。

**headtail：** 在一个瓶颈block的前后分别shuffle和restore。

## 两种STH卷积核的实现

基于pytorch
```python
# STH merge
class STHConv_merge:
    def __init__(self, Ci, Co):
        self.conv_T = torch.nn.Conv3d(Ci, Co // 4, (3,1,1))
        self.conv_S = torch.nn.Conv3d(Ci, Co * 3 // 4, (1,3,3))

    def forward(self, x): # x: [B, Ci, T, H, W]
        B, Ci, T, H, W = x.shape
        O_T = self.conv_T(x) # [B, Co/4, T, H, W]
        O_S = self.conv_S(x) # [B, Co*3/4, T, H, W]
        return torch.cat((O_T, O_S]), dim=1) # [B, Co, T, H, W]

# STH hybrid
class STHConv_hybrid:
    def __init__(self, Ci, Co):
        self.conv_T = torch.nn.Conv3d(Ci, Co, (3,1,1), groups=4)
        self.conv_S_1 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))
        self.conv_S_2 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))
        self.conv_S_3 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))
        self.conv_S_4 = torch.nn.Conv3d(Ci, Co // 4, (1,3,3))

    def forward(self, x): # x: [B, Ci, T, H, W]
        B, Ci, T, H, W = x.shape

        O_T = self.conv_T(x) # [B, Co, T, H, W]

        x1 = x[:,Ci//4:, :, :, :] # [B, Ci/4, T, H, W]
        x2 = torch.cat((x[:,:Ci//4, :, :, :], x[:,Ci*2//4:, :, :, :], dim=1) # [B, Ci/4, T, H, W]
        x3 = torch.cat((x[:,:Ci*2//4, :, :, :], x[:,Ci*3//4:, :, :, :], dim=1) # [B, Ci/4, T, H, W]
        x4 = x[:,:Ci*3//4, :, :, :] # [B, Ci/4, T, H, W]
        O_S_1 = self.conv_S_1(x1) # [B, Co/4, T, H, W]
        O_S_2 = self.conv_S_2(x2) # [B, Co/4, T, H, W]
        O_S_3 = self.conv_S_3(x3) # [B, Co/4, T, H, W]
        O_S_4 = self.conv_S_4(x4) # [B, Co/4, T, H, W]
        O_S = torch.cat((O_S_1, O_S_2, O_S_3, O_S_4), dim=1) # [B, Co, T, H, W]
        return O_S + O_T
```

## temporal信息与spatial信息的attention融合

如果我们把spatial卷积核卷出来的特征图和temporal卷积核卷出的的特征图分开来看，分别记作<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{O}^{S}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{O}^{S}" title="{O}^{S}" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{O}^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{O}^{T}" title="{O}^{T}" /></a>。之前所述的默认做法其实是将这两个特征图直接element-wise相加，得到的特征图就是STH卷积核卷出的特征图。但是如果我们想进一步增强模型对时序信息和空间信息的权衡，可以不做element-wise相加，而是attention融合：

![融合1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/integration1.JPG)

attention参数的计算如下图所示：

![融合2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/integration2.JPG)

后文中，文章用了一个很有意思的方式来验证attention的重要性。

# 实验

## inference

**Something-Something V1:** 中心crop，单clip

**Kinetics400&UCF101&HMDB（不依赖时序信息数据集）:** 10crop，10clip

**Something-Something V2&Jester（依赖时序信息数据集）:** 中心crop，2clip

## performence

![p1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/performence1.JPG)

![p2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/performence2.JPG)

![p3](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/performence3.JPG)

在大部分依赖时序信息的数据集上，STH做到了当时的SOTA，且FLOPS和Params显著的低，甚至低于TSN。只在Jester的top 1上略低于TSM。

在Kinetics上，仍然打不过3D网络。

## runtime

![runtime](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/runtime.JPG)

尽管FLOPS数低，但是由于卷积形状的特殊性，目前肯定做不到很好的加速兼容，因此速度略逊于TSM。

## Attention

文章统计并可视化了不同情况下的attention系数的分布情况（计算方式是简单的将attention系数向量加和平均），来观察模型对于时序信息和空间信息的注意力分布。

![att1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/att1.JPG)

![att2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-17-PR-STH/att2.JPG)


可视化结果令人舒适，可得出如下结论：
1. **模型层数越深，时序信息越重要（<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;{\alpha}^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;{\alpha}^{T}" title="{\alpha}^{T}" /></a>的求和平均越大）**
2. p越小，即STH卷积核中的temporal卷积核比重越低，模型越会倾向于更多地关注时序信息来补足temporal卷积核的匮乏。
3. 在越依赖时序信息的数据集上，模型越关注时序信息。
4. 当使用光流输入时，模型比起使用RGB输入更关注时序信息。