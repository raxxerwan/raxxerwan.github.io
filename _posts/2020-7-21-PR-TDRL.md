---
layout: post
title: 【Paper Reading】TDRL
---

[**Temporal Distinct Representation Learning for Action Recognition**](https://arxiv.org/abs/2007.07626v1)

# 动机

TDRL解决的痛点是视频帧当中大量的重复信息所带来的问题。由于视频帧中大量的帧的背景信息其实几乎是相同的。帧与帧之间的背景大量、重复地出现，而人物动作变化所占的比例非常小。加上CNN网络的卷积核（尤其是2D网络）只是在提取视频帧的spatial信息。这样造成的后果是，网络很容易倾向于通过视频的背景去学习视频的分类（比如简单的通过篮球场背景来将视频分类为打篮球，而不管人物的动作是什么样的），说白了就是严重的过拟合。这一点可以通过TSN在Kinetics数据集上有优秀的识别率，而在Something-Something这样严重依赖时序信息的数据集上表现却异常糟糕来映证。在Something-Something这样的数据集中，每个视频段的背景都没有显著的特征（以日常室内居多），因此TSN几乎完全丧失了识别能力。

TDRL的做法是：
1. 加入时序模块PEM，加强2D网络对时序信息的建模。通过加强2D网络对时序信息的提取能力来加强2D模型的分类能力，这也是目前的视频识别主流。
2. 加入TDLoss对模型进行监督，让模型倾向于过滤掉视频帧中大量重复的背景信息，而只关注视频帧中人物的动作信息。其实光流的作用也是过滤背景关注动作，但是光流的提取实在太慢，基本上不能工业应用。

# 结构

## Progressive Enhancement Module（PEM模块）

![structure](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/structure.JPG)

图中左边的部分就是PEM模块的示意图。做法是SENet的改进，就是算出一个1 * 1 * C的向量来对输入特征图的不同通道进行加权（加强or抑制）：

![form1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/form1.JPG)

很容易理解，<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;X_t^{b-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;X_t^{b-1}" title="X_t^{b-1}" /></a>就是第t帧输入对应的第b-1个block输出的特征图，<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;U_t^b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;U_t^b" title="U_t^b" /></a>就是第t帧输入对应的第b个block的输入特征图，<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;a_t^b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;a_t^b" title="a_t^b" /></a>就是SE向量，维度为1 * 1 * C。

这个SE向量的计算方法则体现了PEM模块是如何对时序信息进行建模的：

![form2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/form2.JPG)

![form3](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/form3.JPG)

![form4](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/form4.JPG)

其中<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;x_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;x_t" title="x_t" /></a>是<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;X_t^{b-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;X_t^{b-1}" title="X_t^{b-1}" /></a>的Global Average Pooling（GAP）。<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;f_1,f_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;f_1,f_2" title="f_1,f_2" /></a>分别是两个1 * 1的卷积。**这个做法我个人感觉非常类似于LSTM了。**<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;d_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;d_t" title="d_t" /></a>使用前后两帧的差来计算而不是单纯的使用第t帧来算，是因为在[**TEINet**](https://arxiv.org/abs/1911.09435v1)中指出，在视频任务的channel-level enhancement learning中，使用前后两帧作为输入学到的SE向量的效果比只用一震作为输入学到的SE向量的效果更好。

另外，<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;m_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;m_0" title="m_0" /></a>可以是全0。但是作者发现选<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;d_{T-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;d_{T-1}" title="d_{T-1}" /></a>效果更好。

## Temporal Diversity Loss（TD-Loss）

![form2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/form5.JPG)

这里面<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;z_i^c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;z_i^c" title="z_i^c" /></a>是第i帧的某一层的特征图的第c个通道向量化后的结果，其维度为1 * HW。<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\eta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\eta" title="\eta" /></a>即余弦相似度。<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\mathbb{I}=\{(i,j)|i\neq&space;j,0\leqslant&space;i,j\leqslant&space;T\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\mathbb{I}=\{(i,j)|i\neq&space;j,0\leqslant&space;i,j\leqslant&space;T\}" title="\mathbb{I}=\{(i,j)|i\neq j,0\leqslant i,j\leqslant T\}" /></a>。那<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;|\mathbb{I}|" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;|\mathbb{I}|" title="|\mathbb{I}|" /></a>就是<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;C^2_T" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;C^2_T" title="C^2_T" /></a>了。这个Loss的目的就是让任意两帧学到的特征图的同一通道的信息尽量不同，结果也就是模型会倾向于滤除重复出现的spatial pattern了。

这样网络的loss即为：

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{L}=\mathcal{L}_c&plus;\lambda&space;\Sigma&space;_{b=1}^{B}\mathcal{L}_\mu&space;^b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{L}=\mathcal{L}_c&plus;\lambda&space;\Sigma&space;_{b=1}^{B}\mathcal{L}_\mu&space;^b" title="\mathcal{L}=\mathcal{L}_c+\lambda \Sigma _{b=1}^{B}\mathcal{L}_\mu ^b" /></a>

B是网络的blocks的数量。

但是，也并非所有的背景信息都要滤除，有些背景信息是有助于网络分类的（比如背景是篮球场的话，那么动作确实有很大概览会是打篮球）。因此TD Loss的计算不会将所有的通道都算在loss里，而是只算一定比例的通道数。在实验中，文章发现这个比例为50%时模型表现最好。

## 怎么加，加在哪

怎么在网络中加入PEM模块和TD loss：

![TDRL](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/TDRLblock.JPG)

以ResNet为backbone，忽略掉res1不计，即layer1为res2，layer2为res3，以此类推：

1. 给ResNet中每一个block都加入PEM模块；
2. 只分别在ResNet的最后三个stage（res3、4、5）的最后一个block加TD Loss（因为时序网络在低层更关注spatial信息，而只在高层更关注temporal信息，这一点在许多论文中都论证了。因此只在高层加TD Loss更好）。
3. TM模块即Temporal Module，如TSM，VSN和TEINet里的MEM。

TDRL-A就是只加PEM的block，TDRL-B就是加了PEM和TD Loss的block。

# 实验

## inference

**Kinetics400：** 10clip，3crop

**Something-Something：** 单clip，单crop

## performence

![p1](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/performence1.JPG)

![p2](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/performence2.JPG)

Something-Something是SotA。相同输入相同backbone下比起TEINet平均涨2点。

值得关注的是，在Kinetics上，ResNet50 backbone下，TDRL已经超过了SlowFast（4 * 16输入），且两者FLOPS接近。不过SlowFast是train from scratch的，如果pretrain on ImageNet，不知道效果如何。

# 探究

## 超参数选择

![ablation](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/ablation.JPG)

值得关注的是，PEM加在TM的前后区别不大，但是TD Loss加在TM的前还是后区别很大。文章的分析是，TM倾向于使前后帧的特征图具有相同的信息（如TSM，VSN都是如此，因为不同帧之间的信息平移了，让帧的信息更加“混乱”、“高斯”了）。这样后续的卷积层就更难从中提取明确的pattern了。而如果在TM后再加TD loss，就能滤波这些“混乱”且“相似”的特征图，让特征图只剩下少量的motion pattern，这样卷积层就能更有效地提取这些pattern。

## 可视化探究

### PEM

![PEMeffect](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/PEMeffect.JPG)

作者选取了res2的第一个卷积block的输出特征图来做可视化。这样特征图人眼还能看得懂。

1. 第一行是input；
2. 第二行是没加PEM的特征图的可视化结果。
3. 第三行是加了PEM之后的特征图中，被增强程度最大的十个通道的可视化结果。
4. 第四行是加了PEM之后的特征图中，被抑制程度最大的十个通道的可视化结果。

这里的“被增强/抑制程度”指的是SE向量中对应通道值的大小，值小的通道被抑制，值大的通道被增强。

可以看到，被增强的通道几乎完全在关注人物的轮廓，也就是motion、temporal信息；而被抑制的通道都是在关注背景信息，对人物的响应很低。这和文章想要达到的目的（过滤大量重复出现的背景）非常吻合。

### TD loss

![TDLosseffect](https://raw.githubusercontent.com/raxxerwan/raxxerwan.github.io/master/images/2020-7-21-PR-TDRL/TDLosseffect.JPG)

从这个可视化结果可以看出来，在加入TD loss之后，特征图对帧之间的变化部分非常敏感，而对不变的部分则非常不敏感。可以从最后两帧画红框的部分明显看出：在加了TD loss之前，TM的输入输出是很相近的；而在加了TD loss之后，特征图通过了TM层后，特征图中前后两帧中没变的部分响应很低，响应基本集中在变化了的部分（抬起的手，前倾的躯干等）。

